{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbFslhiOo01"
      },
      "source": [
        "### Human Detection Using Deeplearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BgiGoIVQ6GNH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.io.image import ImageReadMode\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VHz6oK9a6NNp"
      },
      "outputs": [],
      "source": [
        "test_data_path = \"data\\\\test\"\n",
        "train_data_path = \"data\\\\train\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PsH5KLfV9NX"
      },
      "source": [
        "* Setting device agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KpcTQGa4PHhj"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJKcn5QPWRKs"
      },
      "source": [
        "### Transform Data\n",
        "1. Resize the images to 256x256.\n",
        "2. Flip images randomly on the horizontal.\n",
        "3. Turn our images into tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OUFwUecz6PtU"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256,256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256,256)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bozu_fTRXVog"
      },
      "source": [
        "### Load our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3jFXzLDV6RTi"
      },
      "outputs": [],
      "source": [
        "train_data = datasets.ImageFolder(root=train_data_path, # target folder of images\n",
        "                                  transform=train_transform, # transforms to perform on data (images)\n",
        "                                  target_transform=None)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_data_path, # target folder of images\n",
        "                                  transform=test_transform, # transforms to perform on data (images)\n",
        "                                  target_transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTEZJV2a6UHR",
        "outputId": "8d618369-b22b-4450-f43c-11808f846ee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['human', 'nonhuman']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBAI2FurXc-4"
      },
      "source": [
        "### Turning images into DataLoader's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "37JOC1qy6VND"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=1, # how many samples per batch?\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                              batch_size=1, # how many samples per batch?\n",
        "                              shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek63FkX8YFf7"
      },
      "source": [
        "### Create a TinyVGG CNN architecture\n",
        "\n",
        "The TinyVGG architecture is a smaller and more efficient version of the VGG-16 and VGG-19 convolutional neural networks. TinyVGG has the following key features:\n",
        "\n",
        "* It uses smaller convolutional filters (3x3 instead of 5x5) and fewer convolutional layers .\n",
        "* It uses max pooling instead of average pooling.\n",
        "\n",
        "These changes make TinyVGG significantly smaller and faster than VGG-16 and VGG-19, while still maintaining good accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7DgRyVrb6XWE"
      },
      "outputs": [],
      "source": [
        "class TinyVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv_block_3 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*64*64,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.conv_block_3(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfWd2BVCaGRE"
      },
      "source": [
        "### Training & Test Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RujAqEki6auH"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "    m = nn.Dropout(p=0.2)\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = model(m(X))\n",
        "\n",
        "        # Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou4qzAb9aSVc"
      },
      "source": [
        "### Train & Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S47sqvHU6dKl"
      },
      "outputs": [],
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5):\n",
        "\n",
        "\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn)\n",
        "\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSRVSaR2YmLl"
      },
      "source": [
        "### Initialize the model, loss function and optimizer\n",
        "\n",
        "1. loss function(Cross-entropy loss)\n",
        "\n",
        "  * Cross-entropy loss is a loss function used in machine learning to measure the performance of a classification model.\n",
        "  * It measures the difference between the predicted probability distribution of a machine learning classification model and the true probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nEAhG_Jb6f9f"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels\n",
        "                  hidden_units=5,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSJUyIrAYt_l"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtvW2qvQ6hT5",
        "outputId": "e0463a81-d4f2-4172-9e0e-cb3793f1f579"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5120 and 20480x2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32md:\\Programming languages\\AI\\CV_project\\cvProject.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_0_results \u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mmodel_0,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                         train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                         test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                         epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS)\n",
            "\u001b[1;32md:\\Programming languages\\AI\\CV_project\\cvProject.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                        dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         loss_fn\u001b[39m=\u001b[39mloss_fn)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     )\n",
            "\u001b[1;32md:\\Programming languages\\AI\\CV_project\\cvProject.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(m(X))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Calculate  and accumulate loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32md:\\Programming languages\\AI\\CV_project\\cvProject.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_block_2(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_block_3(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming%20languages/AI/CV_project/cvProject.ipynb#X31sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5120 and 20480x2)"
          ]
        }
      ],
      "source": [
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f96--wZ_Rrn8"
      },
      "source": [
        "## Testing model on custom data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snZ-kMxHRBPj",
        "outputId": "3bcadb16-0935-49e5-a72e-be3702b7a4a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human\n"
          ]
        }
      ],
      "source": [
        "#custom_image_path = \"data\\Sample-Image-captured-by-CCTV-camera.png\"\n",
        "custom_image_path = \"data\\\\2015-04-02_18_21.JPG\"\n",
        "\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path), mode=ImageReadMode.RGB).type(torch.float32)\n",
        "\n",
        "#Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255\n",
        "custom_image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "])\n",
        "\n",
        "# Transform target image\n",
        "custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "    custom_image_pred = model_0(custom_image_transformed.unsqueeze(dim=0).to(device))\n",
        "\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()]\n",
        "\n",
        "print(custom_image_pred_class)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
